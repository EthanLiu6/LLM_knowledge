<h1 align="center"> <p>005_Transformer-Other</p></h1>

> è¯¥ç¯‡ä¸»è¦è®²è§£åœ¨Transformeræž¶æž„åŸºç¡€ä¸Šçš„æ›´å¤šä¼˜åŒ–ï¼ŒåŒ…æ‹¬è½¯ç¡¬ä»¶ä¼˜åŒ–
>
> å­¦ä¹ æ¯ä¸ªå°çŸ¥è¯†ç‚¹çš„æ—¶å€™ä¸€å®šè¦ç«™åœ¨å…¨å±€è§†è§’åŽ»çœ‹çœ‹ä»–å¤„åœ¨ä»€ä¹ˆä½ç½®ï¼Œè¦è§£å†³ä»€ä¹ˆé—®é¢˜

æƒ³æƒ³Transofrmeræœ‰å“ªäº›éƒ½éœ€è¦ä¼˜åŒ–å‘¢ï¼Ÿ

> Since being introduced seven years agoï¼ˆä¸è¿‡çŽ°åœ¨25å¹´æ˜¯ç¬¬å…«å¹´äº†ï¼‰, many modifications to the Transformer architecture have been proposed. However, relatively few of them generalize well across domains and scales and have seen widespread adoption (Narang et al., 2021) Some notable successful ones include Transformer-XL (Dai et al., 2019) and Rotary Position Encoding (Su et al., 2024) for improving long-context handling and position encoding, GLU MLP (Shazeer, 2020) and Sparse Mixture-of-Experts (MoE) MLP (Lepikhin et al., 2020; Fedus et al., 2022) for more expressive or efficient MLP nonlinearty and architecture, UL2 (Tay et al., 2022) and GLM (Du et al., 2021) for better training objectives. Among these, RoPE and SwiGLU MLP have been adopted by recent well-known foundation models such as Palm (Chowdhery et al., 2023) and LLaMA (Touvron et al., 2023), and are also used as our strong baseline (Transformer++).ï¼ˆè¿˜æœ‰MoEï¼‰



## 1. KV-cache

åœ¨ç”Ÿæˆå¼Transformerä¸­ï¼Œç¼“å­˜(Caching) Key(K)å’Œ Value(V)çŠ¶æ€çš„æŠ€æœ¯å·²ç»å­˜åœ¨ä¸€æ®µæ—¶é—´äº†ã€‚è¿™ç§æŠ€æœ¯å¯ä»¥æ˜¾è‘—æé«˜æŽ¨ç†é€Ÿåº¦ï¼Œå›žæƒ³ä¸€ä¸‹æ³¨æ„åŠ›æœºåˆ¶ï¼ŒKeyå’ŒValueçŠ¶æ€ç”¨äºŽè®¡ç®—å¸¦ç¼©æ”¾çš„ç‚¹ç§¯æ³¨æ„åŠ›æœºåˆ¶(scaled dot-product attention)ã€‚

KV Cacheå‘ç”Ÿåœ¨å¤šä¸ªtokensç”Ÿæˆæ­¥éª¤ä¸­ï¼Œåªåœ¨Decoderä¸­è¿›è¡Œï¼ˆå³åœ¨ä»…è§£ç å™¨çš„æ¨¡åž‹å¦‚GPTã€LLamaã€DeepSeekç­‰ä¸­ï¼Œæˆ–è€…åœ¨ç¼–ç å™¨-è§£ç å™¨æ¨¡åž‹å¦‚T5ä¸­çš„è§£ç å™¨éƒ¨åˆ†ï¼‰ã€‚åƒBERTè¿™æ ·çš„æ¨¡åž‹ä¸æ˜¯ç”Ÿæˆæ¨¡åž‹ï¼Œå› æ­¤æ²¡æœ‰KV Cacheã€‚ï¼ˆä¸“é—¨æœ‰ä¸€ç« èŠ‚è¿›è¡Œæ•´ç†LLMå±‚é¢çš„åŸºäºŽTransformeræž¶æž„çš„å¤§è‡´åˆ†æžï¼‰

è§£ç å™¨ä»¥è‡ªå›žå½’(auto-regressive)çš„æ–¹å¼å·¥ä½œï¼Œå°±åƒä¸‹å›¾GPT-2æ–‡æœ¬ç”Ÿæˆç¤ºä¾‹æ‰€ç¤ºçš„é‚£æ ·ã€‚

![figure1](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/68747470733a2f2f6d69726f2e6d656469756d2e636f6d2f76322f726573697a653a6669743a313130302f666f726d61743a776562702f302a7365784f3661644768614b72376148302e676966.gif)

> *åœ¨Decoderçš„è‡ªå›žå½’ç”Ÿæˆä¸­ï¼Œç»™å®šä¸€ä¸ªè¾“å…¥ï¼Œæ¨¡åž‹ä¼šé¢„æµ‹ä¸‹ä¸€ä¸ªtokenï¼Œç„¶åŽåœ¨ä¸‹ä¸€æ­¥ä¸­ä½¿ç”¨ç»„åˆçš„è¾“å…¥è¿›è¡Œä¸‹ä¸€ä¸ªé¢„æµ‹ã€‚)*

è¿™ç§è‡ªå›žå½’è¡Œä¸ºä¼šé‡å¤(repeats)ä¸€äº›æ“ä½œï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ”¾å¤§(zoom in) Decoder ä¸­è®¡ç®—çš„å¸¦æŽ©ç çš„ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›(masked scaled dot-product attention)æ¥æ›´å¥½åœ°ç†è§£è¿™ä¸€ç‚¹ã€‚

![figure2](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/kv-cache-gif1.gif)

> *(Decoderä¸­ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›çš„é€æ­¥å¯è§†åŒ–ã€‚emb_sizeè¡¨ç¤ºembedding size.)*

æ ¹æ®åŠ¨å›¾å¯ä»¥ç›´è§‚å‘çŽ°å…¶ä¸­å­˜åœ¨å¤§é‡çš„å†—ä½™è®¡ç®—ï¼Œæ¯ç”Ÿæˆä¸€ä¸ªtokenéœ€é‡æ–°è®¡ç®—æ‰€æœ‰åŽ†å²tokençš„Key/Valueï¼Œå¤æ‚åº¦ä¸º $O(n^2)$ ï¼Œæ˜¾å­˜å’Œè®¡ç®—æ—¶é—´éšåºåˆ—é•¿åº¦æ€¥å‰§å¢žé•¿ï¼Œæ¯”å¦‚ï¼š

- ç”Ÿæˆembeddingæœ‰å†—ä½™è®¡ç®—ã€‚ï¼ˆåŠ¨å›¾é‡Œé¢æ²¡å±•ç¤ºï¼‰
- KVç”Ÿæˆæœ‰å†—ä½™è®¡ç®—ã€‚
- $QK^T$æœ‰å†—ä½™è®¡ç®—ã€‚
- softmaxæ“ä½œä»¥åŠä¸ŽVç›¸ä¹˜æœ‰å†—ä½™è®¡ç®—ã€‚

~~è¿™ç§ä¼˜åŒ–ä¸ºä»€ä¹ˆé‡è¦å‘¢ï¼Ÿå¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œä½¿ç”¨KVç¼“å­˜å¾—åˆ°çš„çŸ©é˜µè¦å°å¾—å¤šï¼Œè¿™å¯¼è‡´çŸ©é˜µä¹˜æ³•æ›´å¿«ã€‚å”¯ä¸€çš„ç¼ºç‚¹æ˜¯å®ƒéœ€è¦æ›´å¤šçš„GPU VRAMï¼ˆæˆ–è€…å¦‚æžœæ²¡æœ‰ä½¿ç”¨GPUï¼Œåˆ™éœ€è¦æ›´å¤šçš„CPU RAMï¼‰æ¥ç¼“å­˜é”®(Key)å’Œå€¼(Value)çš„çŠ¶æ€ã€‚~~

**æ€è€ƒï¼š**

- å¦‚æžœå¼•å…¥ç¼“å­˜ï¼Œåœ¨å“ªå—åšç¼“å­˜å‘¢ï¼Ÿå¦‚ä½•åšå‘¢ï¼Ÿ

### 1.1 Splitwise

åœ¨"Splitwise: Efficient generative llm inference using phase splitting."è®ºæ–‡ä¸­ï¼Œæœ‰ç›¸å…³çš„ä¸€äº›åˆ†æžå’Œä¼˜åŒ–ï¼Œæˆ‘ä»¬å…ˆæ¥çœ‹çœ‹è¿™ç¯‡paperã€‚

åœ¨è®ºæ–‡æ‘˜è¦éƒ¨åˆ†å°±æŒ‡å‡ºï¼ŒLLMåœ¨æŽ¨ç†è¿‡ç¨‹ä¸­å¾€å¾€åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼š

> ä¸€ä¸ªè®¡ç®—å¯†é›†åž‹çš„promptè®¡ç®—é˜¶æ®µå’Œä¸€ä¸ªå†…å­˜å¯†é›†åž‹çš„tokenç”Ÿæˆé˜¶æ®µï¼Œæ¯ä¸ªé˜¶æ®µéƒ½æœ‰ä¸åŒçš„å»¶è¿Ÿã€‚

![image-20250508162535149](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250508162535149.png)

è®ºæ–‡ä¸­çš„ç›´è§‚å›¾ç‰‡ï¼š

![image-20250508162933736](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250508162933736.png)

ä»–ä»¬æ‰€åšçš„å·¥ä½œï¼š

> **æˆ‘ä»¬çš„å·¥ä½œã€‚** é‰´äºŽç”Ÿæˆå¤§è¯­è¨€æ¨¡åž‹çš„æŽ¨ç†è¯·æ±‚çš„è®¡ç®—é˜¶æ®µæ¸…æ™°ï¼Œæˆ‘ä»¬å»ºè®®å°†æç¤ºè®¡ç®—å’Œè¯ç¬¦ç”Ÿæˆé˜¶æ®µåˆ†ç¦»åˆ°ä¸åŒçš„æœºå™¨ä¸Šã€‚ è¿™æé«˜äº†ç¡¬ä»¶åˆ©ç”¨çŽ‡ï¼Œä»Žè€Œæé«˜äº†ç³»ç»Ÿçš„æ•´ä½“æ•ˆçŽ‡ã€‚ å®ƒè¿˜å…è®¸ä¸ºæ¯ä¸ªé˜¶æ®µä½¿ç”¨ä¸åŒçš„ã€é…ç½®æ›´å¥½çš„ç¡¬ä»¶ã€‚ä¸ºäº†å®žçŽ°è¿™æ ·çš„è®¾ç½®ï¼Œæ¥è‡ªæç¤ºè®¡ç®—çš„ç¼“å­˜ä¸Šä¸‹æ–‡å¿…é¡»ä»¥ä½Žå»¶è¿Ÿä»Žæç¤ºæœºä¼ é€åˆ°è¯ç¬¦æœºã€‚ æˆ‘ä»¬ä½¿ç”¨å¯ç”¨å¸¦å®½ä»¥ä¼˜åŒ–çš„æ–¹å¼å®žçŽ°è¿™äº›ä¼ è¾“ï¼Œè¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿæé«˜æ•ˆçŽ‡ï¼Œè€Œä¸ä¼šé€ æˆä»»ä½•æ˜Žæ˜¾çš„æ€§èƒ½æŸå¤±ã€‚

å…¶å®žå¯ä»¥çœ‹å¾—è§ï¼Œå›¾ç‰‡é‡Œé¢å·²ç»å±•ç¤ºäº†KV cacheï¼Œä¸‹é¢æˆ‘ä»¬ä¸»è¦å›´ç»•KV cacheè®²è§£ï¼Œè¿™ç¯‡è®ºæ–‡æš‚æ—¶å…ˆçœ‹åˆ°è¿™é‡Œå°±å¯ä»¥äº†ï¼Œè¿™é‡Œä¸»è¦æ˜¯å¼•å‡ºä¸€ä¸‹å®è§‚è®¤è¯†ã€‚æœ€åŽï¼Œæˆ‘å¼•ç”¨ä¸€ä¸‹æŸ³æµ©è€å¸ˆçš„åšå®¢å†…å®¹ï¼Œå†™çš„çœŸçš„å¾ˆå¥½

å¯¹äºŽæœªä½¿ç”¨KV cacheçš„æƒ…å†µï¼š

> æˆ‘ä»¬å¯¹ä¸¤ä¸ªé˜¶æ®µçš„ç‰¹ç‚¹è¿›è¡Œæ·±å…¥åˆ†æžã€‚
>
> 1. prompt phaseï¼ˆé¢„å¡«å……é˜¶æ®µï¼‰ï¼Œä¹Ÿæœ‰å«å¯åŠ¨é˜¶æ®µï¼ˆinitiation phaseï¼‰ï¼Œå…¶ç‰¹ç‚¹å¦‚ä¸‹ï¼š
>
> - æ—¶æœºï¼šå‘ç”Ÿåœ¨è®¡ç®—ç¬¬ä¸€ä¸ªè¾“å‡º token è¿‡ç¨‹ä¸­ã€‚
> - è¾“å…¥ï¼šè¾“å…¥ä¸€ä¸ªpromptåºåˆ—ã€‚
> - ä½œç”¨ï¼šä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰çš„ç”¨æˆ·è¾“å…¥ã€‚LLMså¯¹è¾“å…¥åºåˆ—ï¼ˆå³è¾“å…¥æç¤ºï¼‰çš„ä¸Šä¸‹æ–‡è¿›è¡Œæ€»ç»“ï¼Œå¹¶ç”Ÿæˆä¸€ä¸ªæ–°æ ‡è®°ä½œä¸ºè§£ç é˜¶æ®µçš„åˆå§‹è¾“å…¥ã€‚
> - æ‰§è¡Œæ¬¡æ•°ï¼šå…¶é€šè¿‡ä¸€æ¬¡ Forward å°±å¯ä»¥å®Œæˆã€‚ï¼ˆ**ç¬”è€…è§‰å¾—æŸ³è€å¸ˆå§è¿™ç‚¹æå‡ºæ¥çœŸçš„å¾ˆæ¸…æ™°äº†**ï¼‰
> - è®¡ç®—ç±»åž‹ï¼šå­˜åœ¨å¤§é‡ GEMM (GEneral Matrix-Matrix multiply) æ“ä½œï¼Œå±žäºŽ Compute-bound ç±»åž‹ï¼ˆè®¡ç®—å¯†é›†åž‹ï¼‰è®¡ç®—ã€‚
> - å¹¶è¡Œï¼šè¾“å…¥çš„Tokensä¹‹é—´ä»¥å¹¶è¡Œæ–¹å¼æ‰§è¡Œè¿ç®—ï¼Œæ˜¯ä¸€ç§é«˜åº¦å¹¶è¡ŒåŒ–çš„çŸ©é˜µæ“ä½œï¼Œå…·å¤‡æ¯”è¾ƒé«˜çš„æ‰§è¡Œæ•ˆçŽ‡ã€‚
>
> > æƒ³æƒ³ï¼Œåœ¨è®­ç»ƒé˜¶æ®µæ˜¯ä¸æ˜¯ç±»ä¼¼è¿™ç§æƒ…å†µï¼Ÿ
>
> 2. token-generation phaseçš„ç‰¹ç‚¹å¦‚ä¸‹ï¼š
>
> - æ—¶æœºï¼šåœ¨prompté˜¶æ®µç”Ÿæˆç¬¬ä¸€ä¸ª Tokenä¹‹åŽï¼Œå¼€å§‹è¿›å…¥token-generation phaseé˜¶æ®µã€‚å‘ç”Ÿåœ¨è®¡ç®—ç¬¬äºŒä¸ªè¾“å‡º token è‡³æœ€åŽä¸€ä¸ª token è¿‡ç¨‹ä¸­ã€‚
> - è¾“å…¥ï¼šæ–°ç”Ÿæˆçš„tokenä¼šä¸Žè¾“å…¥tokensï¼ˆè¿™é‡ŒåŒ…æ‹¬ç”¨æˆ·è¾“å…¥çš„promptï¼‰ æ‹¼æŽ¥åœ¨ä¸€èµ·ï¼Œä½œä¸ºä¸‹ä¸€æ¬¡æŽ¨ç†çš„è¾“å…¥ã€‚
> - ä½œç”¨ï¼šæ–°ç”Ÿæˆçš„tokenè¢«åé¦ˆå›žè§£ç é˜¶æ®µä½œä¸ºè¾“å…¥ï¼ˆåº”è¯¥å°±æ˜¯æ–°çš„tokenå¯¹åº”çš„embeddingï¼‰ï¼Œä»Žè€Œåˆ›å»ºäº†ä¸€ä¸ªç”¨äºŽtokenç”Ÿæˆçš„è‡ªå›žå½’è¿‡ç¨‹ã€‚
> - æ‰§è¡Œæ¬¡æ•°ï¼šå‡è®¾è¾“å‡ºæ€»å…±æœ‰ N ä¸ª Tokenï¼Œåˆ™ token-generation phaseé˜¶æ®µéœ€è¦æ‰§è¡Œ N-1 æ¬¡ Forwardã€‚
> - è®¡ç®—ç±»åž‹ï¼šå­˜åœ¨å¤§é‡è®¿å­˜æ“ä½œï¼Œå±žäºŽ Memory-bound ç±»åž‹ï¼ˆå†…å­˜å¯†é›†åž‹ï¼‰è®¡ç®—ã€‚
> - å¹¶è¡Œï¼šå‡è®¾è¾“å‡ºæ€»å…±æœ‰ N ä¸ª Tokenï¼Œåˆ™ Decoding é˜¶æ®µéœ€è¦æ‰§è¡Œ N-1 æ¬¡ Forwardï¼Œè¿™ N-1 æ¬¡ Forward åªèƒ½ä¸²è¡Œæ‰§è¡Œï¼Œå› æ­¤æ•ˆçŽ‡ç›¸å¯¹æ¯”è¾ƒä½Žã€‚å¦å¤–ï¼Œåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œéœ€è¦å…³æ³¨çš„ Token è¶Šæ¥è¶Šå¤šï¼ˆæ¯ä¸ª Token çš„ç”Ÿæˆéƒ½éœ€è¦ Attention ä¹‹å‰çš„ Tokenï¼‰ï¼Œè®¡ç®—é‡ä¹Ÿä¼šé€‚å½“å¢žå¤§ã€‚
>
> è‡ªå›žå½’çš„ç”Ÿæˆæ¨¡å¼æ˜¯ä¸¤é˜¶æ®µçš„æ ¹æœ¬åŽŸå› ï¼Œä¸¤é˜¶æ®µæ˜¯è‡ªå›žå½’çš„ç”Ÿæˆæ¨¡å¼çš„å¤–åœ¨ä½“çŽ°å½¢å¼ï¼ŒKV cacheæ˜¯ä¼˜åŒ–æ‰‹æ®µã€‚
>
> æ³¨ï¼šåœ¨SplitWiseè®ºæ–‡ä¸­ï¼Œåˆ†åˆ«æŠŠè¿™ä¸¤ä¸ªé˜¶æ®µç§°ä¸ºprompt phase å’Œ token-generation phaseã€‚åœ¨å®žè·µä¸­ï¼Œâ€œé¢„å¡«å……ï¼ˆpre-fillï¼‰â€å’Œâ€œåˆå§‹åŒ–ï¼ˆinitiationï¼‰â€è¿™ä¸¤ä¸ªæœ¯è¯­å¯ä»¥äº’æ¢ã€‚ä¸ºäº†æ›´å¥½çš„è¯´æ˜Žï¼ŒçŽ°åœ¨æˆ‘ä»¬å°†æ›´å€¾å‘äºŽä½¿ç”¨å‰è€…ã€‚

å¯¹äºŽä½¿ç”¨KV cacheçš„æƒ…å†µï¼š

> 

### 1.2



### 1.n KV cacheç¨€ç–åŒ–

> https://zhuanlan.zhihu.com/p/704710823

## 2. MHAçš„ä¼˜åŒ–

å›žæƒ³ä¸€ä¸‹MHAçš„æµç¨‹

![img](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/1850883-20250308125852651-1423435728.jpg)



### 2.1 MQA

- èƒŒæ™¯ï¼š
        MQAï¼ˆMulti Query Attentionï¼‰æœ€æ—©æ˜¯å‡ºçŽ°åœ¨2019å¹´è°·æ­Œçš„ä¸€ç¯‡è®ºæ–‡ [ã€ŠFast Transformer Decoding: One Write-Head is All You Needã€‹](https://arxiv.org/pdf/1911.02150)ï¼Œä¹‹æ‰€ä»¥æ²¡æœ‰è¢«å…³æ³¨åˆ°ï¼Œæ˜¯å› ä¸ºæ–‡æœ¬ç”Ÿæˆç±»ä»»åŠ¡è¿˜æ²¡è¿™ä¹ˆç«çƒ­ï¼Œè§£ç åºåˆ—é•¿åº¦ä¹Ÿæ²¡æœ‰çŽ°é˜¶æ®µå¤§æ¨¡åž‹çš„è¦æ±‚é‚£ä¹ˆé«˜ã€‚

- æ ¸å¿ƒæ€æƒ³ï¼š
        MQA è®©æ‰€æœ‰çš„å¤´ä¹‹é—´ å…±äº« åŒä¸€ä»½ Key å’Œ Value çŸ©é˜µï¼Œæ¯ä¸ªå¤´åªå•ç‹¬ä¿ç•™äº†ä¸€ä»½ Query å‚æ•°ï¼Œä»Žè€Œå¤§å¤§å‡å°‘ Key å’Œ Value çŸ©é˜µçš„å‚æ•°é‡ã€‚

    > Multi-query attention is identical except that the different heads share a single set of keys and values.

    ![image-20250507113148432](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250507113148432.png)

- ç›´æŽ¥çœ‹å›¾

    ![image-20250507113448111](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250507113448111.png)

**æ€è€ƒï¼š**

- æ¢åšæ˜¯ä½ ï¼Œä½ ä¼šå¯¹MHAï¼ˆå¤šå¤´æ³¨æ„åŠ›ï¼‰åšæ€Žæ ·çš„ä¼˜åŒ–å‘¢ï¼Ÿ
- å¯¹äºŽMHAç³»åˆ—ï¼ˆå„ç§ä¼˜åŒ–ï¼‰**å¤šä¸ªå¤´ç›´æŽ¥æ‹¼æŽ¥çš„æ“ä½œï¼Œ ç›¸å½“äºŽé»˜è®¤äº†æ¯ä¸ªå¤´æˆ–è€…è¯´æ¯ä¸ªå­ç©ºé—´çš„é‡è¦æ€§æ˜¯ä¸€æ ·çš„ï¼Œ åœ¨æ¯ä¸ªå­ç©ºé—´é‡Œé¢å­¦ä¹ åˆ°çš„ç›¸ä¼¼æ€§çš„é‡è¦åº¦æ˜¯ä¸€æ ·çš„ï¼Œå³è¿™äº›å¤´çš„æƒé‡æ˜¯ä¸€æ ·çš„**ã€‚ç„¶è€Œï¼Œå„ä¸ªå¤´çš„æƒé‡äº‹å®žä¸Šè‚¯å®šä¸åŒï¼Œå¦‚ä½•æœ‰æœºèžåˆï¼Ÿæˆ–è€…è¯´ï¼Œå¦‚ä½•è°ƒæ•´ä¸åŒå¤´ä¹‹é—´çš„æƒé‡æ¯”ä¾‹ï¼Ÿ

### 2.2 Uptraining

> converting the checkpoint and uptraining

- å¼•å…¥

    (**uptraining** æ˜¯æŒ‡å¯¹å·²æœ‰çš„æ¨¡åž‹è¿›è¡Œè¿›ä¸€æ­¥çš„è®­ç»ƒ(pre-train)æˆ–å¾®è°ƒ(fine-tune)ã€‚å®ƒå¯ä»¥æ˜¯ä¸ºäº†é€‚åº”æ–°çš„ä»»åŠ¡æˆ–ç»“æž„ï¼Œæˆ–è€…æ”¹è¿›æ¨¡åž‹çš„æ€§èƒ½ã€‚åœ¨è¿™é‡Œï¼Œ **uptraining** æ˜¯æŒ‡å°†å…·æœ‰å¤šå¤´æ³¨æ„åŠ›çš„è¯­è¨€æ¨¡åž‹è½¬æ¢ä¸ºå…·æœ‰å¤šæŸ¥è¯¢æ³¨æ„åŠ›çš„æ¨¡åž‹ï¼Œå¹¶é€šè¿‡é¢å¤–çš„é¢„è®­ç»ƒé˜¶æ®µæ¥é€‚åº”æ–°çš„ç»“æž„ã€‚)

    ä¹Ÿå°±æ˜¯è¯´ï¼Œ**uptraining** å…¶å®žæ˜¯ä¸ºäº†ä¼˜åŒ–MQAçš„ï¼ˆMQAæœ‰å•¥é—®é¢˜ï¼Ÿï¼‰

- æ¦‚å¿µ
    åœ¨ Multi-Query Attention æ–¹æ³•ä¸­åªä¼šä¿ç•™ä¸€ä¸ªå•ç‹¬çš„key-valueå¤´ï¼Œè¿™æ ·è™½ç„¶å¯ä»¥æå‡æŽ¨ç†çš„é€Ÿåº¦ï¼Œä½†æ˜¯ä¼šå¸¦æ¥ç²¾åº¦ä¸Šçš„æŸå¤±ã€‚ã€ŠMulti-Head Attention:Collaborate Instead of Concatenate ã€‹è¿™ç¯‡è®ºæ–‡çš„ç¬¬ä¸€ä¸ªæ€è·¯æ˜¯åŸºäºŽå¤šä¸ª MQA çš„ checkpoint è¿›è¡Œ finetuningï¼Œæ¥å¾—åˆ°äº†ä¸€ä¸ªè´¨é‡æ›´é«˜çš„ MQA æ¨¡åž‹ã€‚è¿™ä¸ªè¿‡ç¨‹ä¹Ÿè¢«ç§°ä¸º Uptrainingã€‚

- ä»Žå¤šå¤´æ¨¡åž‹ç”Ÿæˆå¤šæŸ¥è¯¢æ¨¡åž‹åˆ†ä¸ºä¸¤ä¸ªæ­¥éª¤ï¼š

    - é¦–å…ˆæ˜¯è½¬æ¢æ£€æŸ¥ç‚¹(checkpoint)ï¼Œå°†å¤šå¤´æ£€æŸ¥ç‚¹è½¬æ¢ä¸ºå¤šæŸ¥è¯¢æ£€æŸ¥ç‚¹ã€‚keyå’Œvalueå¤´çš„æŠ•å½±çŸ©é˜µè¢«å¹³å‡æ±‡æ€»ä¸ºå•ä¸ªæŠ•å½±çŸ©é˜µï¼Œæˆ‘ä»¬å‘çŽ°è¿™æ¯”é€‰æ‹©å•ä¸ªé”®å’Œå€¼å¤´æˆ–ä»Žå¤´å¼€å§‹éšæœºåˆå§‹åŒ–æ–°çš„é”®å’Œå€¼å¤´æ•ˆæžœæ›´å¥½ã€‚
    - è½¬æ¢åŽçš„æ£€æŸ¥ç‚¹æŽ¥ç€ä½¿ç”¨ç›¸åŒçš„é¢„è®­ç»ƒæ–¹æ³•è¿›è¡Œé¢„è®­ç»ƒï¼Œä½†ä»…è¿›è¡ŒåŽŸå§‹è®­ç»ƒæ­¥éª¤çš„ä¸€å°éƒ¨åˆ†Î±ã€‚

- å›¾ç¤º

    ![figure21](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/gqa-figure1.jpg)



vanilla Transformerä¸­ï¼Œå¯¹äºŽä¸åŒçš„æ³¨æ„åŠ›é‡‡å–çš„æ•´åˆæ–¹å¼æ˜¯ç›´æŽ¥æ‹¼æŽ¥ã€‚è®ºæ–‡"Multi-Head Attention: Collaborate Instead of Concatenateâ€œæå‡ºäº†å…¶å®ƒæ•´åˆæ–¹å¼ã€‚è¯¥è®ºæ–‡å‘çŽ°æ‰€æœ‰æ³¨æ„åŠ›å¤´ä¹‹é—´æ•æ‰çš„ä¿¡æ¯è‚¯å®šæ˜¯å­˜åœ¨å†—ä½™çš„ï¼Œå¤´ä¸Žå¤´ä¹‹é—´å­˜åœ¨è¾ƒå¤šçš„é€šç”¨ä¿¡æ¯ã€‚æ‹¼æŽ¥åŽçš„ $ð‘Š_ð‘„$$ð‘Š_ð¾^T$ åªéœ€è¦å¤§æ¦‚1/3çš„ç»´åº¦å°±è¶³å¤Ÿæ•æ‰ç»å¤§éƒ¨åˆ†çš„ä¿¡æ¯äº†ã€‚å› æ­¤è®ºæ–‡ä½œè€…è®¾è®¡äº†ä¸€ä¸ªæ··åˆå‘é‡æ¥æå–æ³¨æ„åŠ›å¤´ä¹‹é—´çš„é€šç”¨ä¿¡æ¯ã€‚è¿™ä¸ªå‘é‡å¯ä»¥é€šè¿‡è·Ÿæ¨¡åž‹ä¸€èµ·å­¦ä¹ å¾—åˆ°ï¼Œç„¶åŽåº”ç”¨åˆ°åŽŸå§‹çš„å¤šå¤´æ³¨æ„åŠ›è®¡ç®—ä¸­ã€‚è¿™ç§æ–¹æ¡ˆå¯ä»¥è®©æ³¨æ„åŠ›å¤´çš„è¡¨ç¤ºæ–¹å¼æ›´åŠ çµæ´»ï¼Œæ³¨æ„åŠ›å¤´çš„ç»´åº¦å¯ä»¥æ ¹æ®å®žé™…æƒ…å†µè¿›è¡Œæ”¹å˜ã€‚ä¹Ÿè®©å‚æ•°è®¡ç®—æ›´åŠ é«˜æ•ˆã€‚

![image-20250507153800973](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250507153800973.png)

### 2.3 GQAï¼ˆå¤§æ¨¡åž‹ç¥žå™¨ï¼‰

ä»ŽMHAåˆ°MQAï¼Œé€Ÿåº¦çš„ç¡®æ˜¯æé«˜äº†ï¼Œä½†æ˜¯è´¨é‡å¾ˆå¤§å¯èƒ½æ˜¯é™ä½Žäº†çš„ï¼ŒåŽæ¥Googleåœ¨2023å¹´åˆå‘è¡¨äº†ç›¸å…³çš„ä¸€ç¯‡è®ºæ–‡[GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints](https://arxiv.org/pdf/2305.13245)

- è®ºæ–‡æ‘˜è¦ï¼ˆç¿»è¯‘åŽï¼‰

    > å¤šæŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆMQAï¼‰ä»…ä½¿ç”¨å•ä¸ªé”®å€¼å¤´ï¼Œæžå¤§åœ°åŠ å¿«äº†è§£ç å™¨æŽ¨ç†é€Ÿåº¦ã€‚ ç„¶è€Œï¼ŒMQAå¯èƒ½ä¼šå¯¼è‡´è´¨é‡ä¸‹é™ï¼Œè€Œä¸”ä»…ä»…ä¸ºäº†æ›´å¿«çš„æŽ¨ç†è€Œè®­ç»ƒä¸€ä¸ªå•ç‹¬çš„æ¨¡åž‹å¯èƒ½å¹¶ä¸ç†æƒ³ã€‚ æˆ‘ä»¬ï¼ˆ1ï¼‰æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œä½¿ç”¨åŽŸå§‹é¢„è®­ç»ƒè®¡ç®—é‡çš„5%å°†çŽ°æœ‰çš„å¤šå¤´è¯­è¨€æ¨¡åž‹æ£€æŸ¥ç‚¹å‡çº§ä¸ºå…·æœ‰MQAçš„æ¨¡åž‹ï¼›ï¼ˆ2ï¼‰å¼•å…¥äº†åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆGQAï¼‰ï¼Œå®ƒæ˜¯å¤šæŸ¥è¯¢æ³¨æ„åŠ›çš„æ³›åŒ–ï¼Œå®ƒä½¿ç”¨ä¸­é—´æ•°é‡ï¼ˆå¤šäºŽä¸€ä¸ªï¼Œå°‘äºŽæŸ¥è¯¢å¤´çš„æ•°é‡ï¼‰çš„é”®å€¼å¤´ã€‚ **æˆ‘ä»¬è¡¨æ˜Žï¼Œç»è¿‡è®­ç»ƒçš„ GQA è¾¾åˆ°äº†æŽ¥è¿‘å¤šå¤´æ³¨æ„åŠ›çš„è´¨é‡ï¼Œå¹¶ä¸”é€Ÿåº¦ä¸Ž MQA ç›¸å½“ã€‚**

- æ ¸å¿ƒç»“æž„

    GQAï¼ˆGrouped Query Attentionï¼‰å°†æŸ¥è¯¢å¤´åˆ†æˆGä¸ªç»„ï¼Œæ¯ä¸ªç»„å…±äº«ä¸€ä¸ªé”®å¤´å’Œå€¼å¤´ã€‚GQA-Gè¡¨ç¤ºå…·æœ‰Gä¸ªç»„çš„åˆ†ç»„æŸ¥è¯¢ã€‚GQA-1è¡¨ç¤ºå•ä¸ªç»„ï¼Œå› æ­¤å…·æœ‰å•ä¸ªé”®å¤´å’Œå€¼å¤´ï¼Œç­‰æ•ˆäºŽMQAã€‚è€ŒGQA-Hè¡¨ç¤ºç»„æ•°ç­‰äºŽå¤´æ•°ï¼Œç­‰æ•ˆäºŽMHAã€‚ä¸‹å›¾æ˜¾ç¤ºäº†åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›å’Œå¤šå¤´/å¤šæŸ¥è¯¢æ³¨æ„åŠ›çš„æ¯”è¾ƒã€‚åœ¨å°†å¤šå¤´æ£€æŸ¥ç‚¹è½¬æ¢ä¸ºGQAæ£€æŸ¥ç‚¹æ—¶ï¼Œæˆ‘ä»¬é€šè¿‡å¯¹è¯¥ç»„å†…æ‰€æœ‰åŽŸå§‹å¤´è¿›è¡Œå¹³å‡æ±‡æ€»æ¥æž„å»ºæ¯ä¸ªç»„çš„é”®å¤´å’Œå€¼å¤´ã€‚

    ![image-20250507114518347](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250507114518347.png)

å¤§åž‹æ¨¡åž‹çš„MHAä¼šå°†å•ä¸ªé”®å’Œå€¼å¤´å¤åˆ¶åˆ°æ¨¡åž‹åˆ†åŒºçš„æ•°é‡ï¼ŒMQAä»£è¡¨äº†å†…å­˜å¸¦å®½å’Œå®¹é‡çš„æ›´å¤§å¹…åº¦çš„å‰Šå‡ï¼Œè€ŒGQA ä½¿æˆ‘ä»¬èƒ½å¤Ÿéšç€æ¨¡åž‹å¤§å°çš„å¢žåŠ ä¿æŒå¸¦å®½å’Œå®¹é‡çš„ç›¸åŒæ¯”ä¾‹ä¸‹é™ï¼Œå¯ä»¥ä¸ºè¾ƒå¤§çš„æ¨¡åž‹æä¾›ç‰¹åˆ«å¥½çš„æƒè¡¡ã€‚GQA æ¶ˆé™¤äº†è¿™ç§åˆ†ç‰‡å¸¦æ¥çš„æµªè´¹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬é¢„è®¡ GQA å°†ä¸ºè¾ƒå¤§çš„æ¨¡åž‹æä¾›ç‰¹åˆ«å¥½çš„æƒè¡¡ã€‚

![img](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/1850883-20250413203221768-1617176553.jpg)



GQAåœ¨æŽ¨ç†é˜¶æ®µå¯ä»¥æ˜¾è‘—é™ä½ŽKV Cacheçš„å¤§å°ï¼Œä¸ºæ›´å¤§çš„ Batch Size æä¾›äº†ç©ºé—´ï¼Œå¯ä»¥è¿›ä¸€æ­¥æå‡åžåã€‚

**æ€è€ƒï¼š**

- è™½ç„¶æœ€æ–°çš„æ¨¡åž‹åŸºæœ¬éƒ½åœ¨é¢„è®­ç»ƒé˜¶æ®µé»˜è®¤é‡‡ç”¨ GQAï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥æ€è€ƒä¸‹ï¼Œå¦‚ä½•å°†å·²ç»è®­ç»ƒå¥½çš„MHAç»“æž„çš„æ¨¡åž‹è½¬æ¢æˆMQAæˆ–è€…GQAï¼Ÿ

### 2.4 MOHSA

Transformeræ¨¡åž‹æˆåŠŸçš„ä¸»è¦åŽŸå› æ˜¯ä¸åŒ Token ä¹‹é—´çš„æœ‰æ•ˆä¿¡æ¯äº¤æ¢ï¼Œä»Žè€Œä½¿æ¯ä¸ª Token éƒ½èƒ½èŽ·å¾—ä¸Šä¸‹æ–‡çš„å…¨å±€è§†å›¾ã€‚ç„¶è€Œï¼Œæ¯ä¸ªHeadä¸­çš„ Query ã€ Keyå’ŒValue æ˜¯åˆ†å¼€çš„ï¼Œæ²¡æœ‰é‡å ï¼Œå½“åœ¨å„ä¸ªHeadä¸­è®¡ç®—æ³¨æ„åŠ›æ—¶ä¹Ÿæ²¡æœ‰ä¿¡æ¯äº¤æ¢ã€‚æ¢å¥è¯è¯´ï¼Œåœ¨è®¡ç®—å½“å‰Headçš„æ³¨æ„åŠ›æ—¶ï¼Œå®ƒæ²¡æœ‰å…¶ä»–Headä¸­çš„ä¿¡æ¯ã€‚å°½ç®¡ Token åœ¨æ³¨æ„åŠ›ä¹‹åŽä¼šé€šè¿‡çº¿æ€§æŠ•å½±è¿›è¡Œå¤„ç†ï¼Œä½†é‚£æ—¶çš„ä¿¡æ¯äº¤æ¢ä»…é™äºŽæ¯ä¸ª Tokenã€‚

è®ºæ–‡â€œ[Improving Vision Transformers by Overlapping Heads in Multi-Head Self-Attention](https://arxiv.org/pdf/2410.14874)â€å°±å¯¹æ­¤è¿›è¡Œäº†ç ”ç©¶ã€‚ä½œè€…æå‡ºä¿¡æ¯äº¤æ¢åœ¨è§†è§‰ Transformer ï¼ˆVision Transformersï¼‰çš„æ³¨æ„åŠ›è®¡ç®—è¿‡ç¨‹ä¸­å¯ä»¥æé«˜æ€§èƒ½ã€‚è¿™å¯ä»¥é€šè¿‡å°†æ¯ä¸ªHeadçš„ queriesã€keyså’Œvaluesä¸Žç›¸é‚»Headçš„ queriesã€keyså’Œvaluesé‡å æ¥å®žçŽ°ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åä¸ºMOHSAï¼ˆMulti-Overlapped-Head Self-Attention/å¤šé‡å å¤´è‡ªæ³¨æ„åŠ›ï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡é‡å Headæ¥æ”¹è¿›å¤šHeadè‡ªæ³¨æ„åŠ›ï¼ˆMulti-Head Self-Attentionï¼‰æœºåˆ¶ï¼Œä½¿å¾—åœ¨è®¡ç®—æ³¨æ„åŠ›æ—¶ï¼Œæ¯ä¸ªHeadä¸­çš„ Qã€ Kå’Œ Vä¹Ÿå¯ä»¥è¢«å…¶ç›¸é‚»Headçš„ Qã€ Kå’Œ Væ‰€å½±å“ï¼ŒHeadé—´ä¿¡æ¯äº¤æµå¯ä»¥ä¸ºè§†è§‰ Transformer å¸¦æ¥æ›´å¥½çš„æ€§èƒ½ã€‚å¦‚å›¾æ‰€ç¤ºã€‚

![image-20250507161610401](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250507161610401.png)

- æ‹†åˆ†å¤´çš„æ—¶å€™å°±è¿›è¡Œé‡å å¤„ç†ï¼Œä½¿ç”¨é‡å ï¼ˆSoftï¼‰é™¤è€Œä¸æ˜¯ç›´æŽ¥é™¤

### 2.5 MoH & MoA

> å¥½å§ï¼Œæˆ‘çš„æƒ³æ³•å·²ç»è¢«pkuï¼ˆåŒ—äº¬å¤§å­¦ï¼‰å­¦è€…åœ¨24å¹´10æœˆä»½å‘å¸ƒå•¦ï¼š[MoH: Multi-Head Attention as Mixture-of-Head Attention](https://arxiv.org/pdf/2410.11842)ã€‚æŸ¥é˜…ä¹‹åŽå‘çŽ°ï¼Œ22å¹´å°±å·²ç»æœ‰ç±»ä¼¼çš„MoAè®ºæ–‡å‘è¡¨åœ¨äº†ACLä¸Šäº†ï¼š[Mixture of Attention Heads: Selecting Attention Heads Per Token](https://aclanthology.org/2022.emnlp-main.278.pdf)

![image-20250507162412593](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250507162412593.png)

![image-20250507163127509](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250507163127509.png)

### 2.6 DCMHA

> [Improving Transformers with Dynamically Composable Multi-Head Attention](https://arxiv.org/pdf/2405.08553)
>
> æš‚æ—¶ä¸è¡¥å……äº†

### 2.7 MLAï¼ˆDeepSeekï¼‰

> DeepSeekçš„å•ç‹¬æŠ€æœ¯ç« èŠ‚ä¹Ÿä¼šè®²è§£
>
> [è®ºæ–‡ï¼šDeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](https://arxiv.org/pdf/2405.04434)
>
> 2.1. Multi-Head Latent Attention: Boosting Inference Efficiency

#### 2.7.1 èƒŒæ™¯

![image-20250507193411670](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250507193411670.png)

ä¸»è¦ä¹Ÿæ˜¯ä¸ºäº†è§£å†³æŽ¨ç†æ—¶å€™KV cacheè¿‡å¤§çš„é—®é¢˜ï¼Œä»¥åŠå®žçŽ°æ›´å¥½çš„æ•ˆæžœã€‚ä¹Ÿæ˜¯åœ¨MQAå’ŒGQAï¼ˆå¯å‡å°‘kv cacheï¼‰çš„é—®é¢˜ä¸Šè¿›è¡Œäº†è‡ªå·±çš„åˆ›æ–°æ€§çš„æ”¹è¿›ã€‚

#### 2.7.2 åŸºæœ¬æ€æƒ³

> ç¬”è€…ä¸ªäººè§‰å¾—æ˜¯å€Ÿé‰´äº†LoRAçš„ä½Žç§©åŽ‹ç¼©æ€æƒ³ï¼Œä½†è¯¥æ€æƒ³åº”ç”¨åœ¨MHAçš„æ—¶å€™è¿˜æ˜¯ä¼šå¼•å‡ºå¾ˆå¤šé—®é¢˜ï¼Œå°±æ­¤ï¼ŒDeepSeekå›¢é˜Ÿè®¾è®¡å‡ºäº†MLAã€‚

- **å…ˆå›žé¡¾ä¸€ä¸‹MHA**

![image-20250507194202385](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250507194202385.png)

- **æˆ‘ä»¬å†çœ‹çœ‹MLAçš„æ€æƒ³**

![image-20250507202319273](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250507202319273.png)

- è®­ç»ƒæ—¶æœŸå¯¹queriesä¹Ÿè¿›è¡Œäº†Low-Rankå¤„ç†

#### 2.7.3 ç»“æž„å›¾å¯¹æ¯”

![image-20250507194435654](https://coderethan-1327000741.cos.ap-chengdu.myqcloud.com/blog-pics/image-20250507194435654.png)

- æ•´ä½“æ€æƒ³

    > 

    MLAï¼ˆMulti-Head Latent Attention / å¤šå¤´æ½œåœ¨æ³¨æ„åŠ›ï¼‰çš„åŸºæœ¬æ€æƒ³æ˜¯å°†æ³¨æ„åŠ›è¾“å…¥$h_t$ åŽ‹ç¼©æˆä¸€ä¸ªä½Žç»´çš„æ½œåœ¨å‘é‡ $c_t^{KV}$ ï¼Œç»´åº¦ä¸º$d_c$ï¼Œä¸”$d_c$è¿œå°äºŽåŽŸå§‹çš„ç»´åº¦ï¼ˆ$h_n d_h$ï¼‰ã€‚åœ¨éœ€è¦è®¡ç®—æ³¨æ„åŠ›æ—¶ï¼Œå¯å°†è¿™ä¸ªæ½œåœ¨å‘é‡$c_t^{KV}$æ˜ å°„å›žé«˜ç»´ç©ºé—´ã€‚å› æ­¤ï¼Œåªéœ€è¦å­˜å‚¨æ½œåœ¨å‘é‡$c_t^{KV}$ï¼Œå°±å¯ä»¥æ˜¾è‘—å‡å°‘å†…å­˜çš„å ç”¨ã€‚

    è¿™ä¸ªè¿‡ç¨‹å¯ä»¥é€šè¿‡ä»¥ä¸‹å…¬å¼æ›´æ­£å¼åœ°è¿›è¡Œæè¿°ã€‚å…¶ä¸­$c_t^{KV}$è¡¨ç¤ºæ½œåœ¨å‘é‡ï¼›$W^{DKV}$æ˜¯åŽ‹ç¼©çŸ©é˜µï¼ˆä¸Šæ ‡ D ä»£è¡¨"ä¸‹æŠ•å½±"ï¼Œå³é™ç»´æ“ä½œï¼‰ï¼Œè´Ÿè´£å°† $h_t$ çš„ç»´åº¦ä»Žï¼ˆ$h_nâ‹…d_h$ï¼‰åŽ‹ç¼©åˆ°$d_c$ï¼›$W^{UK}$å’Œ $W^{UV}$ æ˜¯ä¸ŠæŠ•å½±çŸ©é˜µï¼Œè´Ÿè´£å°†å…±äº«çš„æ½œåœ¨å‘é‡ $c_t^{KV}$ æ˜ å°„å›žé«˜ç»´ç©ºé—´ã€‚åªéœ€è¦å­˜å‚¨è¿™ä¸ªæ½œåœ¨å‘é‡ $c_t^{KV}$ ï¼Œå°±èƒ½èŽ·å¾—å¯¹åº”ä¸åŒæ–‡æœ¬ç‰¹å¾çš„Keyå’ŒValueï¼Œè€Œä¸éœ€è¦å¯¹æ¯ä¸ªæ–‡æœ¬ç‰¹å¾éƒ½å­˜å‚¨å¯¹åº”çš„Keyå’ŒValueã€‚

    ç±»ä¼¼åœ°ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥å°†æŸ¥è¯¢å‘é‡æ˜ å°„åˆ°ä¸€ä¸ªæ½œåœ¨çš„ä½Žç»´å‘é‡ï¼Œç„¶åŽå†å°†å…¶æ˜ å°„å›žåŽŸå§‹çš„é«˜ç»´ç©ºé—´ã€‚è€Œä¸”ï¼ŒMLAåˆç»“åˆäº†æƒé‡å¸æ”¶æŠ€æœ¯ï¼Œå‡å°‘äº†è®¡ç®—å¼€é”€ã€‚

## 2. Flash Attention

> ä¸»è¦è§£å†³Attentionæœºåˆ¶çš„è®¿å­˜ä¼˜åŒ–



## é‡è®¡ç®—



## Page Attention



## vLLM



## 10. ä»£ç å®žçŽ°

### 10.1 MHA

- å“ˆä½›ï¼šâ€œThe Annotated Transformerâ€ä¸­MHAä»£ç çš„å®žçŽ°

```python
def attention(query, key, value, mask=None, dropout=None):
    "Compute 'Scaled Dot Product Attention'"
    d_k = query.size(-1)
    scores = torch.matmul(query, key.transpose(-2, -1)) \
             / math.sqrt(d_k)
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)
    p_attn = F.softmax(scores, dim = -1)
    if dropout is not None:
        p_attn = dropout(p_attn)
    return torch.matmul(p_attn, value), p_attn

class MultiHeadedAttention(nn.Module):
    def __init__(self, h, d_model, dropout=0.1):
        '''
        h: head number
        '''
        super(MultiHeadedAttention, self).__init__()
        assert d_model % h == 0
        # We assume d_v always equals d
        self.d = d_model // h
        self.h = h
        self.linears = clones(nn.Linear(d_model, d_model), 4)
        self.attn = None
        self.dropout = nn.Dropout(p=dropout)
        
    def forward(self, query, key, value, mask=None):
        if mask is not None:
            # Same mask applied to all h heads.
            mask = mask.unsqueeze(1)
        nbatches = query.size(0)
        
        # 1) Do all the linear projections in batch from d_model => h x d 
        query, key, value = \
            [l(x).view(nbatches, -1, self.h, self.d).transpose(1, 2)
             for l, x in zip(self.linears, (query, key, value))]
        
        # 2) Apply attention on all the projected vectors in batch. 
        x, self.attn = attention(query, key, value, mask=mask, 
                                 dropout=self.dropout)
        
        # 3) "Concat" using a view and apply a final linear. 
        x = x.transpose(1, 2).contiguous() \
             .view(nbatches, -1, self.h * self.d)
        return self.linears[-1](x)

```

- llm-foundryï¼šå·¥ä¸šç•Œå®žçŽ°æ–¹å¼

> [æºç åœ°å€](https://github.com/mosaicml/llm-foundry/blob/9c89ab263e72fb9610f28c8ab9cde5d2205b6bff/llmfoundry/models/layers/attention.py)

```python
class MultiheadAttention(nn.Module):
    """Multi-head self attention.

    Using torch or triton attention implemetation enables user to also use
    additive bias.
    """

    def __init__(
        self,
        d_model: int,
        n_heads: int,
        attn_impl: str = 'triton',
        clip_qkv: Optional[float] = None,
        qk_ln: bool = False,
        softmax_scale: Optional[float] = None,
        attn_pdrop: float = 0.0,
        low_precision_layernorm: bool = False,
        verbose: int = 0,
        device: Optional[str] = None,
    ):
        super().__init__()

        self.attn_impl = attn_impl
        self.clip_qkv = clip_qkv
        self.qk_ln = qk_ln

        self.d_model = d_model
        self.n_heads = n_heads
        self.softmax_scale = softmax_scale
        if self.softmax_scale is None:
            self.softmax_scale = 1 / math.sqrt(self.d_model / self.n_heads)
        self.attn_dropout_p = attn_pdrop

        self.Wqkv = nn.Linear(self.d_model, 3 * self.d_model, device=device)
        # for param init fn; enables shape based init of fused layers
        fuse_splits = (d_model, 2 * d_model)
        self.Wqkv._fused = (0, fuse_splits)  # type: ignore

        if self.qk_ln:
            layernorm_class = LPLayerNorm if low_precision_layernorm else nn.LayerNorm
            self.q_ln = layernorm_class(self.d_model, device=device)
            self.k_ln = layernorm_class(self.d_model, device=device)

        if self.attn_impl == 'flash':
            self.attn_fn = flash_attn_fn
        elif self.attn_impl == 'triton':
            self.attn_fn = triton_flash_attn_fn
        elif self.attn_impl == 'torch':
            self.attn_fn = scaled_multihead_dot_product_attention
        else:
            raise ValueError(f'{attn_impl=} is an invalid setting.')

        self.out_proj = nn.Linear(self.d_model, self.d_model, device=device)
        self.out_proj._is_residual = True  # type: ignore

    def forward(
        self,
        x,
        past_key_value=None,
        attn_bias=None,
        attention_mask=None,
        is_causal=True,
        needs_weights=False,
    ):
        qkv = self.Wqkv(x)

        if self.clip_qkv:
            qkv.clamp_(min=-self.clip_qkv, max=self.clip_qkv)

        query, key, value = qkv.chunk(3, dim=2)

        key_padding_mask = attention_mask

        if self.qk_ln:
            # Applying layernorm to qk
            dtype = query.dtype
            query = self.q_ln(query).to(dtype)
            key = self.k_ln(key).to(dtype)

        context, attn_weights, past_key_value = self.attn_fn(
            query,
            key,
            value,
            self.n_heads,
            past_key_value=past_key_value,
            softmax_scale=self.softmax_scale,
            attn_bias=attn_bias,
            key_padding_mask=key_padding_mask,
            is_causal=is_causal,
            dropout_p=self.attn_dropout_p,
            training=self.training,
            needs_weights=needs_weights,
        )

        return self.out_proj(context), attn_weights, past_key_value

    
    
def scaled_multihead_dot_product_attention(
    query,
    key,
    value,
    n_heads,
    past_key_value=None,
    softmax_scale=None,
    attn_bias=None,
    key_padding_mask=None,
    is_causal=False,
    dropout_p=0.0,
    training=False,
    needs_weights=False,
    multiquery=False,
):
    q = rearrange(query, 'b s (h d) -> b h s d', h=n_heads)
    kv_n_heads = 1 if multiquery else n_heads
    k = rearrange(key, 'b s (h d) -> b h d s', h=kv_n_heads)
    v = rearrange(value, 'b s (h d) -> b h s d', h=kv_n_heads)

    if past_key_value is not None:
        if len(past_key_value) != 0:
            k = torch.cat([past_key_value[0], k], dim=3)
            v = torch.cat([past_key_value[1], v], dim=2)
        past_key_value = (k, v)

    b, _, s_q, d = q.shape
    s_k = k.size(-1)

    if softmax_scale is None:
        softmax_scale = 1 / math.sqrt(d)

    attn_weight = q.matmul(k) * softmax_scale

    if attn_bias is not None:
        _s_q = max(0, attn_bias.size(2) - s_q)
        _s_k = max(0, attn_bias.size(3) - s_k)
        attn_bias = attn_bias[:, :, _s_q:, _s_k:]
        attn_weight = attn_weight + attn_bias

    min_val = torch.finfo(q.dtype).min

    if key_padding_mask is not None:
        attn_weight = attn_weight.masked_fill(
            ~key_padding_mask.view((b, 1, 1, s_k)), min_val)

    if is_causal and (not q.size(2) == 1):
        s = max(s_q, s_k)
        causal_mask = attn_weight.new_ones(s, s, dtype=torch.float16)
        causal_mask = causal_mask.tril()
        causal_mask = causal_mask.to(torch.bool)
        causal_mask = ~causal_mask
        causal_mask = causal_mask[-s_q:, -s_k:]
        attn_weight = attn_weight.masked_fill(causal_mask.view(1, 1, s_q, s_k),
                                              min_val)

    attn_weight = torch.softmax(attn_weight, dim=-1)

    if dropout_p:
        attn_weight = torch.nn.functional.dropout(attn_weight,
                                                  p=dropout_p,
                                                  training=training,
                                                  inplace=True)

    out = attn_weight.matmul(v)
    out = rearrange(out, 'b h s d -> b s (h d)')

    if needs_weights:
        return out, attn_weight, past_key_value
    return out, None, past_key_value

```

- LLaMAï¼šLLaMAçš„MHAæºç å®žçŽ°ï¼ˆGQAï¼‰

> [æºç åœ°å€](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py)

```python
class LlamaAttention(nn.Module):
    """Multi-headed attention from 'Attention Is All You Need' paper"""

    def __init__(self, config: LlamaConfig, layer_idx: int):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx
        self.head_dim = getattr(config, "head_dim", config.hidden_size // config.num_attention_heads)
        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads  # è¿™é‡Œä½¿ç”¨çš„å°±æ˜¯GQA
        self.scaling = self.head_dim**-0.5
        self.attention_dropout = config.attention_dropout
        self.is_causal = True

        self.q_proj = nn.Linear(
            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias
        )
        self.k_proj = nn.Linear(
            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias
        )
        self.v_proj = nn.Linear(
            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias
        )
        self.o_proj = nn.Linear(
            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias
        )

    def forward(
        self,
        hidden_states: torch.Tensor,
        position_embeddings: Tuple[torch.Tensor, torch.Tensor],
        attention_mask: Optional[torch.Tensor],
        past_key_value: Optional[Cache] = None,
        cache_position: Optional[torch.LongTensor] = None,
        **kwargs: Unpack[FlashAttentionKwargs],
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
        input_shape = hidden_states.shape[:-1]
        hidden_shape = (*input_shape, -1, self.head_dim)

        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)

        cos, sin = position_embeddings
        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)

        if past_key_value is not None:
            # sin and cos are specific to RoPE models; cache_position needed for the static cache
            cache_kwargs = {"sin": sin, "cos": cos, "cache_position": cache_position}
            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)

        attention_interface: Callable = eager_attention_forward
        if self.config._attn_implementation != "eager":
            if self.config._attn_implementation == "sdpa" and kwargs.get("output_attentions", False):
                logger.warning_once(
                    "`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to "
                    'eager attention. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.'
                )
            else:
                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]

        attn_output, attn_weights = attention_interface(
            self,
            query_states,
            key_states,
            value_states,
            attention_mask,
            dropout=0.0 if not self.training else self.attention_dropout,
            scaling=self.scaling,
            **kwargs,
        )

        attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        attn_output = self.o_proj(attn_output)
        return attn_output, attn_weights
```



### 10.2 MQA

![img](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413202905108-376926323.jpg)

```python
# æˆ‘çš„ç‰ˆæœ¬
class MQA(nn.Module):
    def __init__(self, model_dim, head_num):
        super(MQA, self).__init__()
        self.model_dim = model_dim
        self.head_num = head_num
        self.k_dim = self.model_dim // self.head_num

        self.short_cut = nn.Linear(model_dim, model_dim)
        # multi-head query, all query go to an only same key and value
        self.q_k_v = nn.Linear(self.model_dim, self.model_dim + self.k_dim * 2)

    def forward(self, in_x: Tensor):
        ipt_shape = in_x.shape  # (batch_size, seq_len, model_dim)

        short_cut = self.short_cut(in_x)

        # multi-head query, all query go to an only same key and value
        q_k_v = (self.q_k_v(in_x))
        query_head, same_one_key, same_one_value = torch.split(q_k_v, [self.model_dim, self.k_dim, self.k_dim], dim=-1)


        query_head = query_head.reshape(-1, ipt_shape[1], self.head_num, self.k_dim).transpose(1, 2)
        same_one_key = torch.unsqueeze(same_one_key, dim=2).transpose(1, 2)
        same_one_value = torch.unsqueeze(same_one_value, dim=2).transpose(1, 2)

        # MQA
        score = query_head @ same_one_key.transpose(-1, -2) / (self.k_dim ** 2)
        # (bs, head, seq, kd) @ (bs, 1, seq, kd) = (bs, head, seq, seq)
        atten = torch.softmax(score, dim=-1) @ same_one_value

        res = atten.transpose(1, 2).contiguous().view(ipt_shape)

        return score, atten, res, short_cut

```

```python
# åˆ«äººçš„ç‰ˆæœ¬
class MultiQueryAttention(nn.Module):
    """Multi-Query self attention.

    Using torch or triton attention implemetation enables user to also use
    additive bias.
    """

    def __init__(
        self,
        d_model: int,
        n_heads: int,
        attn_impl: str = 'triton',
        clip_qkv: Optional[float] = None,
        qk_ln: bool = False,
        softmax_scale: Optional[float] = None,
        attn_pdrop: float = 0.0,
        low_precision_layernorm: bool = False,
        verbose: int = 0,
        device: Optional[str] = None,
    ):
        super().__init__()

        self.attn_impl = attn_impl
        self.clip_qkv = clip_qkv
        self.qk_ln = qk_ln

        self.d_model = d_model
        self.n_heads = n_heads
        self.head_dim = d_model // n_heads
        self.softmax_scale = softmax_scale
        if self.softmax_scale is None:
            self.softmax_scale = 1 / math.sqrt(self.head_dim)
        self.attn_dropout_p = attn_pdrop

        # NOTE: if we ever want to make attn TensorParallel, I'm pretty sure we'll
        # want to split Wqkv into Wq and Wkv where Wq can be TensorParallel but
        # Wkv shouldn't be TensorParallel
        # - vchiley
        self.Wqkv = nn.Linear(
            d_model,
            d_model + 2 * self.head_dim,
            device=device,
        )
        # for param init fn; enables shape based init of fused layers
        fuse_splits = (d_model, d_model + self.head_dim)
        self.Wqkv._fused = (0, fuse_splits)  # type: ignore

        if self.qk_ln:
            layernorm_class = LPLayerNorm if low_precision_layernorm else nn.LayerNorm
            self.q_ln = layernorm_class(d_model, device=device)
            self.k_ln = layernorm_class(self.head_dim, device=device)

        if self.attn_impl == 'flash':
            self.attn_fn = flash_attn_fn
        elif self.attn_impl == 'triton':
            self.attn_fn = triton_flash_attn_fn
        elif self.attn_impl == 'torch':
            self.attn_fn = scaled_multihead_dot_product_attention
        else:
            raise ValueError(f'{attn_impl=} is an invalid setting.')

        self.out_proj = nn.Linear(self.d_model, self.d_model, device=device)
        self.out_proj._is_residual = True  # type: ignore

    def forward(
        self,
        x,
        past_key_value=None,
        attn_bias=None,
        attention_mask=None,
        is_causal=True,
        needs_weights=False,
    ):
        qkv = self.Wqkv(x)

        if self.clip_qkv:
            qkv.clamp_(min=-self.clip_qkv, max=self.clip_qkv)

        query, key, value = qkv.split(
            [self.d_model, self.head_dim, self.head_dim], dim=2)

        key_padding_mask = attention_mask

        if self.qk_ln:
            # Applying layernorm to qk
            dtype = query.dtype
            query = self.q_ln(query).to(dtype)
            key = self.k_ln(key).to(dtype)

        context, attn_weights, past_key_value = self.attn_fn(
            query,
            key,
            value,
            self.n_heads,
            past_key_value=past_key_value,
            softmax_scale=self.softmax_scale,
            attn_bias=attn_bias,
            key_padding_mask=key_padding_mask,
            is_causal=is_causal,
            dropout_p=self.attn_dropout_p,
            training=self.training,
            needs_weights=needs_weights,
            multiquery=True,
        )

        return self.out_proj(context), attn_weights, past_key_value

```



### 10.3 GQA

```python
pass

```

### 10.4 MLA

```python
pass

```



### 10.5 KV cache

```python
pass

```





## å‚è€ƒèµ„æ–™

MHAç›¸å…³

> https://www.cnblogs.com/rossiXYZ/p/18759167
>
> 

KV cacheç›¸å…³

> Patel, Pratyush, et al. "Splitwise: Efficient generative llm inference using phase splitting." *2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture (ISCA)*. IEEE, 2024.
>
> https://www.cnblogs.com/rossiXYZ/p/18799503







